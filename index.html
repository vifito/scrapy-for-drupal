<!doctype html>
<html lang="gl">

  <head>
    <meta charset="utf-8">

    <title>Scraping de contidos e integración en Drupal 8</title>

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="assets/custom/favicon/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="assets/custom/favicon/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="assets/custom/favicon/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="assets/custom/favicon/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="assets/custom/favicon/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="assets/custom/favicon/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="assets/custom/favicon/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="assets/custom/favicon/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-128.png" sizes="128x128" />
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="assets/custom/favicon/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="assets/custom/favicon/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="assets/custom/favicon/mstile-150x150.png" />
    <meta name="msapplication-square310x310logo" content="assets/custom/favicon/mstile-310x310.png" />

    <meta name="description" content="Scraping de contidos e integración en Drupal 8">
    <meta name="author" content="vifito">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="assets/reveal.js/css/reveal.css">
    <!-- <link rel="stylesheet" href="assets/reveal.js/css/theme/black.css" id="theme"> -->
    <link rel="stylesheet" href="assets/custom/css/usc.css" id="theme">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="assets/reveal.js/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'assets/reveal.js/css/print/pdf.css' : 'assets/reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="assets/reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <section>
            <h1>Scraping de contidos e integración en Drupal8</h1>
            <p>
              <em>Tomás Vilariño Fidalgo</em><br/>

              <small>
                Enxeñeiro en Informática <small>(UVigo)</small><br/>
                Técnico Especialista en Desenvolvemento Web <small>(UJI)</small><br/>
              </small>
            </p>

            <p>
                <a href="https://www.drupal.org/u/vifito"><i class="fa fa-drupal" aria-hidden="true"></i> vifito</a>
                &nbsp;&nbsp;
                <a href="https://github.com/vifito"><i class="fa fa-github" aria-hidden="true"></i> vifito</a>
                &nbsp;&nbsp;
                <a href="https://twitter.com/vifito"><i class="fa fa-twitter" aria-hidden="true"></i>@vifito</a>
            </p>
          </section>

          <!-- <section data-background="assets/custom/images/oficina-web.jpg">
            <h2>Traballo na</h2>
            <h3><i class="fa fa-home" aria-hidden="true"></i> Oficina Web da <abbr title="Universidade de Santiago de Compostela">USC</abbr></h3>
            <p>
              como <br/>Técnico especialista informática <br/> Especialidade desenvolvemento de sistemas
            </p>
          </section> -->
        </section>

        <section>
          <h2>Sobre esta charla</h2>
          <p><img src="assets/custom/images/drupalday2016.png" alt="" data-unbordered="true" /></p>
        </section>

        <section>
          <h2>Contexto</h2>
          <p class="fragment grow">Migración web USC.es</p>
          <p class="fragment grow">Sitio con miles de páxinas</p>
          <p class="fragment grow">Exportar páxinas "estáticas"</p>
          <p class="fragment grow">Complexa exportación datos</p>
          <p class="fragment grow">Manter a mesma estrutura navegación (SEO)</p>
        </section>

        <section>
          <h2>Primeira idea</h2>
          <p>Utilizar solucións de exportación do CMS actual (OpenCMS), ou acceso directo á base de datos</p>
          <p>
            <strong class="fragment fade-in">Problemas</strong>
            <ul>
              <li class="fragment fade-in">Descoñecemento da estrutura da base de datos, elevada curva de aprendizaxe</li>
              <li class="fragment fade-in">O actual CMS emprega un sistema de ficheiros en base de datos (VFS)</li>
              <li class="fragment fade-in">Outros sitios "satélite" que compre integrar (Xornal, Campus Terra, ...)</li>
            </ul>
          </p>
        </section>

        <section>
          <section>
            <h2>Segunda idea</h2>
            <p>Acceder directamente a través da web e extraer os contidos (scraping).</p>
            <blockquote class="left">
              OLLO! falamos de contidos con pouca frecuencia de actualización (case "estáticos"). O resto de contidos son integrados vía servizos web.
            </blockquote>
          </section>

          <section>
            <h3>Proba de concepto I</h3>
            <p>
              Empregar <strong>Goutte</strong> para recuperar e extraer contidos.<br/>Insertar en Drupal a través dun comando <strong>Drush</strong> empregando Entity API.<br/> Todo implementado en <strong>PHP</strong> con componentes alternativos <strong>Guzzle</strong> e <strong>DOMCrawler</strong>.
            </p>
            <p>
              <img src="assets/custom/images/php.png" alt="" border="0" />
            </p>

            <img class="fragment current-visible" src="assets/custom/images/100x100php.png" alt="" style="border-width:0px;box-shadow:none;position:absolute; left:0px; top:0px;" />

          </section>

          <section>
            <h3>Proba de concepto I (inconvintes)</h3>
            <p class="fragment fade-in">
              A idea funciona para unha páxina pero presenta inconvintes para rastrexar todo un sitio.
            </p>
            <p>
              <strong class="fragment fade-in">Faise necesario implementar a maiores:</strong>

              <ul>
                <li class="fragment fade-in">Rastrexo de ligazóns e xestión de duplicados</li>
                <li class="fragment fade-in">Xestión da cola de peticións pendentes</li>
                <li class="fragment fade-in">Almacenamento dos contidos extraídos</li>
                <li class="fragment fade-in">Procesamento multifío</li>
                <li class="fragment fade-in">Xestión jobs: poder parar e reanudar o traballo</li>
                <li class="fragment fade-in">Estatísticas, tratamento imaxes, ficheiros, xestión de redireccións, erros 500...</li>
              </ul>
            </p>

            <aside class="notes">
              - Importante controlar o código de estado e os content-types de retorno
              - Os ficheiros no OpenCMS están no VFS
              - A imaxes están en miniatura (queremos os orixinais)
            </aside>
          </section>

          <section>
            <h3>Proba de concepto II</h3>
            <p>
              Precisamos algo máis que unha API de scraping de páxinas.

              <ul>
                <li class="fragment fade-in">Scraping, extracción de contidos</li>
                <li class="fragment fade-in">Crawling, bot para seguir a navegación</li>
                <li class="fragment fade-in">Storing, almacenar os contidos</li>
                <li class="fragment fade-in">Plumbing, outras tarefas de «fontanería»</li>
              </ul>
            </p>
          </section>

          <section>
            <h3>Proba de concepto II</h3>
            <p>
              <img src="assets/custom/images/scrapy-big-logo.png" data-unbordered="true" alt="" align="right" />
              Atopamos un framework máis completo centrado na programación de arañas.
              <br/>
              Empregamos o framework Scrapy e obtemos contidos en formato JSON. Posteriormente con Drush integrámolos en Drupal.
            </p>
            <p>
              <img src="assets/custom/images/solucionII.png" data-unbordered="true" alt="" border="0" />
            </p>

            <img class="fragment current-visible" src="assets/custom/images/50x50.png" alt="" style="border-width:0px;box-shadow:none;position:absolute; left:0px; top:0px;" />
          </section>
        </section>

        <section>
          <h2>Antes de continuar</h2>
          <p>
            <img src="assets/custom/images/nivel-python.png" data-unbordered="true" alt="" />
          </p>
          <p>
            O meu nivel de Python é baixo (tirando a nulo), porén nunha tarde estás programando arañas.
          </p>
        </section>

        <section>
          <section>
            <h2>Instalación framework scraping (Scrapy)</h2>
            <p>
              Instalación de scrapy empregando <code>pip</code>
            </p>
            <pre><code class="hljs bash" data-trim>
$ sudo pip install scrapy
            </code></pre>

            <p class="fragment fade-in">
              Empregamos o xestor de paquetes de Python
              <a href="https://es.wikipedia.org/wiki/Pip_(administrador_de_paquetes)">
                 pip <i class="fa fa-external-link"></i>
              </a>
              <br/>
              <img src="assets/custom/images/pip.png" data-unbordered="true" alt="" border="0" />
            </p>
          </section>

          <section>
            <h2>Requisitos previos (Ubuntu)</h2>
            <pre><code class="hljs sh">
$ sudo apt-get install python-dev python-pip libxml2-dev \
  libxslt1-dev zlib1g-dev libffi-dev libssl-dev
              </code></pre>
          </section>
        </section>

        <section>
          <section>
            <h2>Probamos a instalación de scrapy</h2>
            <p>
              <img src="assets/custom/images/scrapy-cmd.png" data-unbordered="true" alt="" />
            </p>
          </section>

          <section>
            <h2>Iniciamos proxecto de scrapy</h2>

            <p>
              Creamos un proxecto de scrapy coa opción <code>startproject</code>
            </p>

            <pre><code class="hljs bash">
$ scrapy startproject gdgourense
    </code></pre>
          </section>

          <section>
            <h2>Estrutura do proxecto</h2>
            <pre>
.
├── scrapy.cfg   → Ficheiro de scrapy (nome do proxecto)
└── gdgourense
    ├── __init__.py
    ├── items.py   → Clase coa definición dos ítems a extraer
    ├── pipelines.py   → Clase que procesa, valida, almacena, ... ítems
    ├── settings.py   → Configuración de opcións do proxecto
    └── spiders
        └── __init__.py
            </pre>
          </section>
        </section>

        <section>
          <section>
            <h2>Tipos de arañas</h2>

            <p>
              Dende líña de comandos podemos crear o "esquelete" da araña coa opción <code>genspider</code>.
              Un proxecto pode conter varias arañas.
            </p>

            <p>Scrapy vén con varios modelos para implementar arañas.</p>

            <pre><code class="hljs bash">
$ scrapy genspider --list
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
    </code></pre>
          </section>

          <section>
            <h2>Clases Python</h2>

            <dl>
              <dt>basic</dt>
              <dd>A clase herda de <code>scrapy.Spider</code>, é a implementación máis sinxela. </dd>

              <dt>crawl</dt>
              <dd>A clase herda de <code>scrapy.CrawlSpider</code>. Por medio dunhas regras busca ligazóns e vai enchendo a cola.</dd>

              <dt>xmlfeed</dt>
              <dd>A clase herda de <code>XMLFeedSpider</code>. Deseñada para parsear feeds XML.</dd>

              <dt>csvfeed</dt>
              <dd>A clase herda de <code>CSVFeedSpider</code>. Similar a XMLFeedSpider para documentos CSV.</dd>
            </dl>
          </section>

          <section>
            <h2>Crear unha araña</h2>

            <pre><code class="hljs bash">
$ scrapy genspider -t crawl sitepoint www.sitepoint.com
    </code></pre>
          </section>

          <section>
            <h2>Código da clase xerada</h2>
            <p>
              OLLO! O código xerado pode variar en función da versión de scrapy instalada.
            </p>
            <pre><code class="hljs python"># imports ...
class SitepointSpider(CrawlSpider):
  name = 'sitepoint'
  allowed_domains = ['www.sitepoint.com']
  start_urls = ['http://www.sitepoint.com/']

  rules = (
      Rule(LinkExtractor(allow=r'Items/'), callback='parse_item'),
  )

  def parse_item(self, response):
    i = {}
    #i['domain'] = response.xpath('//input[@id="sid"]/@value').extract()
    #i['name'] = response.xpath('//div[@id="name"]').extract()
    #i['desc'] = response.xpath('//div[@id="description"]').extract()
    return i
</code></pre>
          </section>

          <section>
            <h2>Definimos as nosas regras</h2>

            <pre><code class="hljs python">
rules = (
  Rule(
    LinkExtractor(
        # xpathBasePath é unha expresión regular por liña de comandos
        restrict_xpaths=xpathBasePath,
        deny=[r'/(en|es)/', r'(calMonth=|calYear=)']
      ),
    callback='process_item',
    follow=True
  ),
)
              </code></pre>
          </section>
        </section>

        <section>
          <section>
            <h3>Ítems a extraer (items.py)</h3>

            <p>Definimos a clase <code>PaxinasItem</code> cos campos que imos a recuperar.</p>

            <img src="assets/custom/images/map.png" data-unbordered="true" alt="" />
          </section>

          <section>
            <h3>Selectores</h3>

            <table>
              <thead>
              <tr>
                <th>CSS (response.css)</th>
                <th>XPath (response.xpath)</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <td>title::text</td>
                <td>//title/text()</td>
              </tr>
              <tr>
                <td>base::attr(href)</td>
                <td>//base/@href</td>
              </tr>
              <tr>
                <td>a[href*=image]::attr(href)</td>
                <td>//a[contains(@href, "image")]/@href</td>
              </tr>
              </tbody>
            </table>

            <p>Tamén con expresións regulares <code>response.re()</code>.</p>
          </section>

          <section data-background-video="./assets/custom/scrapy-shell.webm" data-background-color="#FFFFFF">
            <h2 class="heading-inverse">scrapy shell http://...</h2>
          </section>

          <section>
            <h2>Implementamos parse_item</h2>
            <p>Empregamos <code>scrapy shell</code> para facer probas dos selectores</p>

            <pre><code class="hljs python" data-trim>
class PaxinasSpider(CrawlSpider):
  # ....
  def parse_item(self, response):
    # Comprobacións: 'text/html' not in response.headers['Content-Type']

    item = PaxinasItem()
    item['url'] = response.url
    item['title'] = response.xpath('//title/text()').extract_first()
    item['body'] = response.body
    item['keywords'] = response.xpath('//meta[@name="keywords"]/@content')
            .extract_first()
    # Búsqueda de imaxes e ficheiros
    # ....
    item['image_urls'] = images
    item['files_urls'] = files
    return item
            </code></pre>
          </section>
        </section>

        <section>
          <section>
            <h3>Configurar pipelines</h3>

            <p>Precisamos configurar <code>ITEM_PIPELINES</code> para dispoñer do comportamento automático dos campos <code>image_urls</code> e <code>file_urls</code>.</p>

            <pre><code class="hljs python" data-trim>
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 1,
    'scrapy.pipelines.files.FilesPipeline': 1,
}

dir = os.path.dirname(os.path.abspath(__file__))
IMAGES_STORE = dir + '/../assets/images'
FILES_STORE  = dir + '/../assets/files'
            </code></pre>
          </section>

          <section>
            <h4>Funcionamento dun pipeline</h4>
            <img src="assets/custom/images/scrapy_pipeline.png" alt="" data-unbordered="true" />
          </section>
        </section>

        <section>
          <h3>Executamos crawler</h3>
          <p>Lanzamos para que garde en formato json (-o xerencia.json), con parámetros adicionais (-a), incluímos a opción de parar e reanudar o crawler (-s JOBDIR) e gardamos un log.</p>
          <pre><code class="hljs python" data-trim>
scrapy crawl paxinas -o xerencia.json \
    -a start_url='http://www.usc.es/gl/goberno/xerencia/' \
    -a base_path='/goberno/xerencia/' \
    -s JOBDIR=crawls/paxinas-1 \
    --logfile=xerencia.log
          </code></pre>
        </section>

        <section>
          <section>
            <h3>Mapeo de campos</h3>
            <p>Temos que mapear os campos do documento JSON aos campos do tipo de contido.</p>
            <img src="assets/custom/images/map.png" data-unbordered="true" alt="" />
          </section>

          <section>
            <h2>Estrutura JSON</h2>
            <p>(versión reducida)</p>
            <pre><code class="hljs json">[{
  "title": "Servizo de Xestión Académica - USC",
  "url": "http://www.usc.es/gl/servizos/sxa",
  "keywords": "usc, universidade, santiago , compostela, ...",
  "contidos": "<p>...</p>",
  "image_urls": ["http://www.usc.es/gl/servizos/sxa/imaxes/coie.jpg"],
  "file_urls": ["http://www.usc.es/gl/servizos/sxa/4_Vacantes_Master.pdf"],
  "images": [{
      "url": "http://www.usc.es/gl/servizos/sxa/imaxes/coie.jpg",
      "path": "full/7eb49e6658ab0f2e7ed894027529dfaeebd9346a.jpg"}],
  "files": [{
      "url": "http://www.usc.es/gl/servizos/sxa//4_Vacantes_Master.pdf",
      "path": "full/67063aa5e3f072742ce4d2c066b973d1ceca64c8.pdf"}]
}, {...}]</code></pre>
          </section>
        </section>

        <section>
          <section>
            <h1>Integración en Drupal</h1>
          </section>

          <section>
            <h3>Crear un módulo Drupal</h3>
            <p>Ficheiro <code>module_name.drush.inc</code>, contén os comandos que poderemos executar con Drush.</p>
            <pre><code class="hljs php" data-trim>
  /**
   * Implements hook_drush_command().
   */
  function gdgourense_drush_command() {
    $items['import-pages'] = [
      'callback' => 'import_pages',
      'description' => 'Importar páxinas dun sitio (formato JSON).',
      'aliases' => array('ipag'),
      'arguments' => [
        'input' => 'Ficheiro JSON co contido das páxinas a cargar.',],
      'options' => [
        'assets' => 'Ruta ao directorio de assets (files e images)',
        'menu_name' => 'Nome do menú a xerar',
      ],
    ];
  }</code></pre>
          </section>

          <section>
            <h3>Procesamento ficheiro JSON</h3>
            <p>Recorrer ítems do array json e ir inserindo o contido</p>
            <pre><code class="hljs php" data-trim>
  $data = json_decode(file_get_contents($input), TRUE);
  /** @var \Drupal\Core\Path\AliasStorage $aliasStorage */
  $aliasStorage = \Drupal::service('path.alias_storage');

  foreach($data as $item) {
    if (!$aliasStorage->aliasExists($link, $langcode)) {
      $values = [
        'type' => 'page_general',
        'title' => $item['titulo'],
        // ...
        'body' => ['value' => $item['contidos'],],
        'field_description' => $item['description'],
      ];
    }
    // ... tratar imágenes y ficheros
  }</code></pre>
          </section>

          <section>
            <h3>Ficheiros xestionados</h3>
            <p>Recorrer arrays de <code>images</code> e <code>files</code>, e damos de alta en Drupal (táboa: file_managed)</p>
            <pre><code class="hljs php" data-trim>
// Versión reducida: validacións omitidas, file_exists, ...
foreach($item['images'] as $img) {
  $path = 'public://' . $img['url'];
  $imagename = realpath($assets . '/images/' . $img['path']);
  $dir = dirname($path);
  if(!file_prepare_directory($dir)) {
    drupal_mkdir($dir, NULL, True);
  }
  $image_stream = file_get_contents($imagename);
  $file = file_save_data($image_stream, $path, FILE_EXISTS_REPLACE);
  $image_id[] = [
    'target_id' => $file->id(),
    'alt' => $alternativeText,
  ];
  // Remprazar os href no contido ($values['body'])
}
$values['field_image'] = $image_id; // array cos IDs das imáxes
  </code></pre>
          </section>

          <section>
            <h3>Outros campos (taxonomías)</h3>

            <p>Crear taxonomías para o campo <code>keywords</code></p>

            <pre><code class="hljs php" data-trim>
$terms = explode(',', $item['keywords']);
foreach($terms as $term) {
  $termId = $storage->getQuery()
    ->condition('vid', $keywords_vid)
    ->condition('name', $term)->execute();

  if (empty($termId)) {
    $termEntity = \Drupal\taxonomy\Entity\Term::create([
      'vid' => $keywords_vid,
      'name' => $term,
    ]);
    $termEntity->save();
    $termId = [$termEntity->id() => $termEntity->id()];
  }
  $tids[] = ['target_id' => current($termId)];
}
$values['field_keywords'] = $tids;
</code></pre>
          </section>

          <section>
            <h3>Integrar con menús I</h3>

            <p>Comprobar que existe un menú, noutro caso crealo</p>

            <pre><code class="hljs php" data-trim>
// $menu_name => nome do menú
$nids = \Drupal::entityQuery('menu')
  ->condition('id', $menu_name)->execute();

if (empty($nids)) { // Crear menú si no existe
  $menu_desc = t('Menú') . ' ' .
      ucwords(str_replace('menu-', '', $menu_name));
  /** @var \Drupal\system\Entity\Menu $menu */
  $menu = entity_create('menu', [
    'id'          => $menu_name,
    'label'       => $menu_desc,
    'description' => $menu_desc,
    'langcode'    => 'gl',
  ]);
  $menu->save();
}
          </code></pre>
          </section>

          <section>
            <h3>Integrar con menús II</h3>

            <p>Crear ítems do menú</p>

            <pre><code class="hljs php" data-trim>
// Comprobar se xa existe
$nids = \Drupal::entityQuery('menu_link_content')
  ->condition('default_langcode', 1)->condition('title', $item['titulo'])
  ->condition('link.uri', 'internal:' . $link)
  ->condition('menu_name', $menu_name)->execute();

if (count($nids) === 0) {
  $menuData = [
    'title' => $item['titulo'],
    'menu_name' => $menu_name,
    // ...
    'link' => ['uri' => 'internal:' . $link],
  ];

  $menu_link_content = MenuLinkContent::create($menuData);
  $menu_link_content->save();
}
</code></pre>
          </section>
        </section>


        <section>
          <h3>Outras alternativas</h3>
          <p>
            <ul>
              <li>Empregar o módulo <strong>migrate</strong> para cargar os contidos.</li>
              <li class="fragment fade-in">Insertar dende código Python os contidos directamente na <strong>base de datos</strong>.</li>
              <li class="fragment fade-in">Publicar os <strong>servizos REST</strong> de Drupal e dende código Python insertar os contidos empregando o API.</li>
            </ul>
          </p>
        </section>

        <section>
          <section data-background="#000000">
            <h2 class="vermello banned">BANNED</h2>
            <img src="assets/custom/images/terminatorsad.jpg" alt="" data-unbordered="true" />
          </section>

          <section>
            <h2>User-Agent aleatorio</h2>
            <p>Creamos un middleware que escollerá aleatoriamente dunha lista un user-agent distinto en cada petición.</p>
            <pre><code class="hljs python" data-trim>
# ./scrapy/gdgourense/settings.py
USER_AGENT_LIST = [
   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, ...',
   'Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0) Gecko/16.0 ...',
   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/534...'
]

# ./scrapy/gdgourense/middlewares.py
#...
class RandomUserAgentMiddleware(object):
    def process_request(self, request, spider):
        ua  = random.choice(settings.get('USER_AGENT_LIST'))
        if ua:
            request.headers.setdefault('User-Agent', ua)
</code></pre>
          </section>

          <section>
            <h2>Empregar Proxies</h2>
            <p>No caso que nos bloqueen por IP a nosa máquina podemos botar man dos proxies.</p>
            <pre><code class="hljs python" data-trim>
# ./scrapy/gdgourense/settings.py
#HTTP_PROXY = 'http://127.0.0.1:8123' # Polipo proxy service
HTTP_PROXY = 'http://127.0.0.1:8118' # Privoxy

# ./scrapy/gdgourense/middlewares.py
#...
class ProxyMiddleware(object):
    def process_request(self, request, spider):
        request.meta['proxy'] = settings.get('HTTP_PROXY')
</code></pre>
          </section>

          <section>
            <h2>Privoxy e Tor</h2>
            <p class="left">Podemos configurar Scrapy para realice peticións a través de Tor e Privoxy.</p>
            <pre><code class="hljs python" data-trim>
sudo apt-get install tor privoxy
# sudo apt-get install polipo
</code></pre>
            <p class="left">
              <a href="http://www.alcancelibre.org/staticpages/index.php/como-privoxy-tor">
                Configurar Tor e Privoxy
                <i class="fa fa-external-link"></i>
              </a>
            </p>
          </section>

          <section>
            <h2>Solución en scrapinghub</h2>
            <div class="video">
              <iframe src="https://player.vimeo.com/video/135827204?autoplay=0&amp;title=0&amp;byline=0&amp;portrait=0" title="Crawlera" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" data-ready="true" tabindex="-1" width="640" height="360" frameborder="0"></iframe>
            </div>
          </section>
        </section>

        <section>
          <section>
            <h2>Almacenamento alternativo</h2>
            <p>Gardar os contidos en MongoDB para realizar consultas e xerar un JSON máis personalizado.</p>
            <pre><code class="hljs python" data-trim>
import pymongo
# import ...
class PaxinasPipeline(object):
  def __init__(self):
    connection = pymongo.MongoClient(
      settings['MONGODB_SERVER'],
      settings['MONGODB_PORT']
    )
    db = connection[settings['MONGODB_DB']]
    self.collection = db[settings['MONGODB_COLLECTION']]

  def process_item(self, item, spider):
    for data in item:
      if not data:
        raise DropItem("Missing data!")
    self.collection.update({'url': item['url']}, dict(item), upsert=True)
    return item
</code></pre>
            <p></p>
          </section>

          <section>
            <h3>Configuramos pipeline MongoDB</h3>

            <pre><code class="hljs python" data-trim>
ITEM_PIPELINES = {
  'paxinas.pipelines.PaxinasPipeline': 1,
  # ... imaxes, ficheiros, ...
}

MONGODB_SERVER = "localhost"
MONGODB_PORT = 27017
MONGODB_DB = "gdgourense"
MONGODB_COLLECTION = "paxinas"
</code></pre>
          </section>
        </section>

        <section>
          <section>
            <h3>Que pasa coas single-page application (SPA)?</h3>
            <img src="assets/custom/images/fry.png" alt="" data-unbordered="true" />
          </section>

          <section>
            <h2>Splash</h2>
            <p class="left">
              Splash é un <strong>servizo de renderizado javascript</strong>. Algunhas das súas características:
            </p>
            <ul>
              <li>Recuperar o resultado en HTML, ou pantallazo</li>
              <li>Non cargar imaxes ou empregar regras de Adblock Plus para renderizar máis rápido</li>
              <li>Executar Javascript no contexto da páxina</li>
              <li>Scripting en Lua</li>
              <li>Obter captura HAR (HTTP Archive)</li>
            </ul>
            <p class="left">Tamén se pode combinar Scrapy con PhamtonJS ou Selenium.</p>
          </section>

          <section>
            <h2>Splash e Scrapy</h2>
            <p class="left">
              Empregamos o servicio de Splash a través dun
              <a href="https://github.com/scrapy-plugins/scrapy-splash">
                middleware
                <i class="fa fa-external-link"></i>
              </a>.
              As páxinas <em>descárganse e interprétanse</em> con Splash e pásanse a Scrapy.
            </p>
            <pre><code class="hljs python" data-trim>
# ./scrapy/gdgourense/settings.py
SPLASH_URL = 'http://192.168.59.103:8050'
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
</code></pre>
          </section>
        </section>

        <section>
          <section data-background="assets/custom/images/portia.png" data-background-color="#FFFFFF" data-background-size="contain">
            <h2>Portia</h2>
            <p>Portia, proxecto que permite extraer de forma visual.</p>
            <div class="video">
              <iframe src="https://player.vimeo.com/video/129010091?autoplay=0&amp;autopause=1&amp;title=0&amp;byline=0&amp;portrait=0" title="Portia Promotional Video" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" data-ready="true" tabindex="-1" width="640" height="360" frameborder="0"></iframe>
            </div>
          </section>

          <section data-background-color="#FFFFFF" data-background-video="assets/custom/portia.webm">
            <!-- Demo de portia en vídeo -->
          </section>

          <section>
            <h2>Proba portia con docker</h2>
            <pre><code class="hljs bash" data-trim>
docker pull scrapinghub/portia
docker run -i -t --rm -p 9001:9001 --name portia portia

# Abrir no navegador http://127.0.0.1:9001/static/index.html
</code></pre>
          </section>

          <section>
            <h2>Portia2code</h2>
            <p class="left">Converter o proxecto de Portia dentro de arañas Scrapy. Só proxectos con Portia 2 soportados.</p>
            <p class="left">
              <a href="https://blog.scrapinghub.com/2016/06/29/introducing-portia2code-portia-projects-into-scrapy-spiders/">
                Introducing Portia2Code (Blogue Scrapinghub)
                <i class="fa fa-external-link"></i>
              </a>
            </p>
          </section>
        </section>

        <section data-background="#FFFFFF">
          <h2>Indexar contidos</h2>
          <p>
            <img src="assets/custom/images/scrapy-e-solr.png" alt="" data-unbordered="true" />
            <br/>
            <a href="https://github.com/scalingexcellence/scrapy-solr">
              Pipeline de Scrapy para Solr
              <i class="fa fa-external-link"></i>
            </a>
          </p>
        </section>

        <!-- DEMO -->
        <section>
          <section>
            <h3>DEMO</h3>
            <p>
              A demo inclúe <strong>unha araña con Scrapy</strong> que recupera os artigos de
              <a href="https://www.sitepoint.com/tag/drupal-8/">Drupal 8 de Sitepoint <i class="fa fa-external-link"></i></a> e posterior integración nunha instalación limpa de Drupal.
            </p>
            <img src="assets/custom/images/sitepoint-web.png" width="60%" alt="" data-unbordered="true" />
          </section>

          <section>
            <h3>Probando a shell</h3>
            <pre><code class="hljs bash" data-trim>
scrapy shell https://www.sitepoint.com/tag/drupal-8/
</code></pre>
            <img src="assets/custom/images/firebug.png" alt="" data-unbordered="true" />
          </section>

          <section data-background="#FFFFFF">
            <h3>Show me your code</h3>

            <img src="assets/custom/images/Octocat.jpg" data-unbordered="true" alt="" width="20%" />

            <p class="left">
              Esta presentación está dispoñible en:
              <a href="https://vifito.github.io/scrapy-for-drupal/">
                vifito.github.io/scrapy-for-drupal/
                <i class="fa fa-external-link"></i>
              </a>
            </p>

            <p class="left fragment fade-in">
              E no repositorio:
              <a href="https://github.com/vifito/scrapy-for-drupal">
                github.com/vifito/scrapy-for-drupal
                <i class="fa fa-external-link"></i>
              </a>
            </p>

            <pre class="fragment fade-in"><code data-trim>
github.com/vifito/scrapy-for-drupal
└── demo
    ├── module
    │   └── gdgourense
    └── scrapy
        └── gdgourense
              </code></pre>
          </section>

          <section>
            <h3>Contido da Demo</h3>

            <p class="left">
              Un proxecto Scrapy cunha araña de tipo Basic e un <strong>módulo de Drupal</strong> cun comando de Drush para inserir os artigos.
            </p>

            <p class="left fragment fade-in">No directorio <code>scrapy</code> executar <code>launch.sh</code> ou:</p>
            <pre class="fragment fade-in"><code class="hljs bash" data-trim>
$ scrapy crawl sitepoint -o drupal.json --logfile=drupal.log
              </code></pre>

            <p class="left fragment fade-in">Copiar o ficheiro <code>drupal.json</code> e o directorio <code>assets</code> á raíz dunha instalación drupal e executar:</p>
            <pre class="fragment fade-in"><code class="hljs bash" data-trim>
$ drush gdgourense-import drupal.json --assets=./assets
              </code></pre>
          </section>

          <section>
            <h3>Mans a obra</h3>
            <p>A imaxe <code>wadmiraal/drupal</code> contén unha instalación de drupal con drush, drupal console, mailhog, ...</p>
<pre><code class="hljs bash" data-trim>cd scrapy-for-drupal/demo
# Arrincar un contedor
docker run --name gdgourense -d \
    -v `pwd`/module:/var/www/modules/custom \
    -v `pwd`/scrapy:/var/www/scrapy \
    -p 8080:80 wadmiraal/drupal

# Lanzamos Scrapy para cargar contidos
cd scrapy
./launch.sh

# Acceso ao contedor e executar drush
docker exec -it gdgourense bash
cd /var/www/scrapy
drush -y en gdgourense file_entity
drush gdgourense-import `pwd`/drupal.json --assets=`pwd`/assets
</code></pre>
          </section>
      </section>
      <!-- /DEMO -->

        <section>
          <section>
            <h1>Referencias Scrapy</h1>
            <ul class="left">
              <li><a href="https://scrapy.org/">Páxina de Scrapy</a></li>
              <li><a href="https://github.com/scrapinghub/">Proxectos de Scrapinghub en Github (scrapy, portia, portia2code, splash, ...)</a></li>
              <li><a href="http://splash.readthedocs.io/en/stable/">Documentación de Splash</a></li>
              <li><a href="http://pkmishra.github.io/blog/2013/03/18/how-to-run-scrapy-with-TOR-and-multiple-browser-agents-part-1-mac/">Scrapy empregando TOR e alternando User-agent</a></li>
              <li><a href="https://lucidworks.com/2013/06/13/indexing-web-sites-in-solr-with-python/">Indexando sitios con Scrapy e Solr</a></li>
              <li><a href="https://blog.scrapinghub.com/2016/06/29/introducing-portia2code-portia-projects-into-scrapy-spiders/">Proxectos de Portia2code dentro de scrapy</a></li>
              <li><a href="https://github.com/scrapinghub/scrapy-training/blob/master/unit5/README.md">Unit 5: Scraping JavaScript based pages</a></li>
              <li><a href="https://potentpages.com/web-crawler-development/tutorials/scrapy">Titoriais de Scrapy</a></li>
            </ul>
          </section>

          <section>
            <h1>Referencias Drupal 8</h1>
            <ul class="left">
              <li><a href="https://www.drupal.org/">Páxina oficial de Drupal</a></li>
              <li><a href="http://www.drush.org/en/master/">Drush</a></li>
              <li><a href="http://drupalconsole.com">Drupal Console</a></li>
              <li><a href="https://www.drupal.org/docs/8/core/modules/rest">Expoñer entidades como API REST</a></li>
            </ul>
          </section>
        </section>

        <section>
          <h1>Grazas pola atención</h1>
        </section>

      </div>

    </div>

    <script src="assets/reveal.js/lib/js/head.min.js"></script>
    <script src="assets/reveal.js/js/reveal.js"></script>

    <script>

      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: 'c/t',

        transition: 'concave', // none/fade/slide/convex/concave/zoom

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'assets/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'assets/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'assets/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'assets/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'assets/reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'assets/reveal.js/plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
