<!doctype html>
<html lang="gl">

  <head>
    <meta charset="utf-8">

    <title>Scraping de contidos e integración en Drupal 8</title>

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="assets/custom/favicon/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="assets/custom/favicon/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="assets/custom/favicon/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="assets/custom/favicon/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="assets/custom/favicon/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="assets/custom/favicon/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="assets/custom/favicon/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="assets/custom/favicon/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="assets/custom/favicon/favicon-128.png" sizes="128x128" />
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="assets/custom/favicon/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="assets/custom/favicon/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="assets/custom/favicon/mstile-150x150.png" />
    <meta name="msapplication-square310x310logo" content="assets/custom/favicon/mstile-310x310.png" />

    <meta name="description" content="Scraping de contidos e integración en Drupal 8">
    <meta name="author" content="vifito">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="assets/reveal.js/css/reveal.css">
    <!-- <link rel="stylesheet" href="assets/reveal.js/css/theme/black.css" id="theme"> -->
    <link rel="stylesheet" href="assets/custom/css/usc.css" id="theme">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="assets/reveal.js/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'assets/reveal.js/css/print/pdf.css' : 'assets/reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="assets/reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <section>
            <h1>Scraping de contidos e integración en Drupal8</h1>
            <p>
              <em>Tomás Vilariño Fidalgo</em><br/>

              <small>
                Enxeñeiro en Informática <small>(UVigo)</small><br/>
                Técnico Especialista en Desenvolvemento Web <small>(UJI)</small><br/>
              </small>
            </p>

            <p>
                <a href="https://www.drupal.org/u/vifito"><i class="fa fa-drupal" aria-hidden="true"></i> vifito</a>
                &nbsp;&nbsp;
                <a href="https://twitter.com/vifito"><i class="fa fa-twitter" aria-hidden="true"></i>@vifito</a>
            </p>
          </section>

          <section data-background="assets/custom/images/oficina-web.jpg">
            <h2>Traballo na</h2>
            <h3><i class="fa fa-home" aria-hidden="true"></i> Oficina Web da <abbr title="Universidade de Santiago de Compostela">USC</abbr></h3>
            <!-- <p>
              como <br/>Técnico especialista informática <br/> Especialidade desenvolvemento de sistemas
            </p> -->
          </section>
        </section>

        <!--<section>
          <h2>Agenda</h2>
          <ul>
            <li>Situación actual</li>
            <li>Instalación de las herramientas necesarias</li>
            <li>Tipos de crawlers</li>
            <li>Extracción de contenidos</li>
            <li>Limpieza de código HTML, tratamiento imágenes y ficheros</li>
            <li>Integración en Drupal</li>
          </ul>
        </section>-->

        <section>
          <h2>Sobre esta charla</h2>
          <p><img src="assets/custom/images/drupalday2016.png" alt="" /></p>
        </section>

        <section>
          <h2>Situación actual</h2>
          <p class="fragment grow">Migración web USC.es</p>
          <p class="fragment grow">Sitio con miles de páxinas</p>
          <p class="fragment grow">Exportar páxinas "estáticas"</p>
          <p class="fragment grow">Complexa exportación datos</p>
          <p class="fragment grow">Manter a mesma estrutura navegación (SEO)</p>
        </section>

        <section>
          <h2>Primeira idea</h2>
          <p>Utilizar solucións de exportación do CMS actual (OpenCMS), ou acceso directo á base de datos</p>
          <p>
            <strong class="fragment fade-in">Problemas</strong>
            <ul>
              <li class="fragment fade-in">Descoñecemento da estrutura da base de datos, elevada curva de aprendizaxe</li>
              <li class="fragment fade-in">O actual CMS emprega un sistema de ficheiros en base de datos (VFS)</li>
              <li class="fragment fade-in">Outros sitios "satélite" que compre integrar (Xornal, Campus Terra, ...)</li>
            </ul>
          </p>
        </section>

        <section>
          <h2>Segunda idea</h2>
          <p>Acceder directamente a través da web e extraer os contidos (scraping).</p>
          <p>OLLO! falamos de integración dos contidos "estáticos". O resto de contidos son integrados vía servizos web.</p>
        </section>

        <section>
          <h2>Proba de concepto I</h2>
          <p>
            Empregar <strong>Goutte</strong> para recuperar e extraer contidos.<br/>Insertar en Drupal a través dun comando <strong>Drush</strong> empregando Entity API.<br/> Todo implementado en <strong>PHP</strong> con componentes alternativos <strong>Guzzle</strong> e <strong>DOMCrawler</strong>.
          </p>
          <p>
            <img src="assets/custom/images/php.png" alt="" border="0" />
          </p>

          <img class="fragment current-visible" src="assets/custom/images/100x100php.png" alt="" style="border-width:0px;box-shadow:none;position:absolute; left:0px; top:0px;" />

        </section>

        <section>
          <h2>Proba de concepto I (inconvintes)</h2>
          <p class="fragment fade-in">
            A idea funciona para unha páxina pero presenta inconvientes para rastrexar todo un sitio.
          </p>
          <p>
            <strong class="fragment fade-in">Faise necesario implementar:</strong>

            <ul>
              <li class="fragment fade-in">Xestión da cola de peticións pendentes</li>
              <li class="fragment fade-in">Rastrexo de ligazóns e xestión de duplicados</li>
              <li class="fragment fade-in">Almacenamento dos contidos extraídos</li>
              <li class="fragment fade-in">Procesamento multifío</li>
              <li class="fragment fade-in">Xestión jobs: pausing and resuming crawls</li>
              <li class="fragment fade-in">Estatísticas, tratamento imaxes, ficheiros, xestión de redireccións, erros 500...</li>
            </ul>
          </p>

          <aside class="notes">
            - Importante controlar o código de estado e os content-types de retorno
            - Os ficheiros no OpenCMS están no VFS
            - A imaxes están en miniatura (queremos os orixinais)
          </aside>
        </section>

        <section>
          <h2>Proba de concepto II</h2>
          <p>
            Precisamos algo máis que unha API de scraping de páxinas.

            <ul>
              <li class="fragment fade-in">Scraping, extracción de contidos</li>
              <li class="fragment fade-in">Crawling, bot para seguir a navegación</li>
              <li class="fragment fade-in">Storing, almacenar os contidos</li>
              <li class="fragment fade-in">Plumbing, outras tarefas de «fontanería»</li>
            </ul>
          </p>
        </section>

        <section>
          <h2>Proba de concepto II</h2>
          <p>
            <img src="assets/custom/images/scrapy-big-logo.png" data-unbordered="true" alt="" align="right" />
            Atopamos un framework máis completo centrado na programación de arañas.
            <br/>
            Empregamos o framework Scrapy e obtenemos contidos en formato JSON. Posteriormente con Drush os integrámolos en Drupal.
          </p>
          <p>
            <img src="assets/custom/images/solucionII.png" data-unbordered="true" alt="" border="0" />
          </p>

          <img class="fragment current-visible" src="assets/custom/images/50x50.png" alt="" style="border-width:0px;box-shadow:none;position:absolute; left:0px; top:0px;" />
        </section>

        <section>
          <h2>Antes de continuar</h2>
          <p>
            <img src="assets/custom/images/nivel-python.png" data-unbordered="true" alt="" />
          </p>
          <p>
            O meu nivel de Python é baixo (tirando a nulo), sen embargo nunha tarde estás programando arañas.
          </p>
        </section>

        <section>
          <section>
            <h2>Instalación framework scraping (Scrapy)</h2>
            <p>
              Instalación de scrapy empregando <code>pip</code>
            </p>
            <pre><code class="hljs bash" data-trim contenteditable>
sudo pip install scrapy
            </code></pre>

            <p class="fragment fade-in">
              Empregamos o xestor de paquetes de Python <a href="https://es.wikipedia.org/wiki/Pip_(administrador_de_paquetes)">pip</a>
              <br/>
              <img src="assets/custom/images/pip.png" data-unbordered="true" alt="" border="0" />
            </p>
          </section>

          <section>
            <h2>Requisitos previos (Ubuntu)</h2>
            <pre><code class="hljs sh">
sudo apt-get install python-dev python-pip libxml2-dev \
  libxslt1-dev zlib1g-dev libffi-dev libssl-dev
              </code></pre>
          </section>

        </section>



        <section>
          <h2>Probamos a instalación de scrapy</h2>
          <p>
            <img src="assets/custom/images/scrapy-cmd.png" data-unbordered="true" alt="" />
          </p>
        </section>

        <section>
          <h2>Iniciamos proxecto de scrapy</h2>

          <p>
            Creamos un proxecto de scrapy coa opción <code>startproject</code>
          </p>

          <pre><code class="hljs bash">
$ scrapy startproject usc
  </code></pre>
        </section>

        <section>
          <h2>Estrutura do proxecto</h2>
          <pre>
.
├── scrapy.cfg   → Ficheiro de scrapy (nome do proxecto)
└── paxinas
    ├── __init__.py
    ├── items.py   → Clase coa definición dos ítems a extraer
    ├── pipelines.py   → Clase que procesa, valida, almacena, ... ítems
    ├── settings.py   → Configuración de opcións do proxecto
    └── spiders
        └── __init__.py
          </pre>
        </section>

        <section>
          <section>
            <h2>Crear unha araña</h2>

            <p>
              Dende líña de comandos podemos crear o "esquelete" da araña coa opción <code>genspider</code>
            </p>

            <p>Scrapy vén con varios modelos para implementar arañas.</p>

            <pre><code class="hljs bash">
  $ scrapy genspider --list
  Available templates:
    basic
    crawl
    csvfeed
    xmlfeed
    </code></pre>
          </section>

          <section>
            <h2>Tipos de crawlers</h2>

            <dl>
              <dt>basic</dt>
              <dd>A clase herda de <code>scrapy.Spider</code>, é a implementación máis sinxela. </dd>

              <dt>crawl</dt>
              <dd>A clase herda de <code>scrapy.CrawlSpider</code>. Por medio dunhas regras busca ligazóns e vai enchendo a cola.</dd>

              <dt>xmlfeed</dt>
              <dd>A clase herda de <code>XMLFeedSpider</code>. Deseñada para parsear feeds XML.</dd>

              <dt>csvfeed</dt>
              <dd>A clase herda de <code>XMLFeedSpider</code>. Similar a XMLFeedSpider para documentos CSV.</dd>
            </dl>
          </section>
        </section>

        <section>
          <section>
            <h2>Crear unha araña II</h2>

            <pre><code class="hljs bash">
$ scrapy genspider -t crawl paxinas
    </code></pre>
          </section>

          <section>
            <h2>Código da clase xerada</h2>

            <pre><code class="hljs python">
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class PaxinasSpider(CrawlSpider):
  name = 'paxinas'
  allowed_domains = ['usc.es']
  start_urls = ['http://www.usc.es/']
  rules = (
    Rule(LinkExtractor(allow=r'Items/'),
          callback='parse_item', follow=True),)

  def parse_item(self, response):
    pass
</code></pre>
          </section>

          <section>
            <h2>Definimos as nosas regras</h2>

            <pre><code class="hljs python">
rules = (
  Rule(
    LinkExtractor(
        # xpathBasePath es una expresión regular por línea de comandos
        restrict_xpaths=xpathBasePath,
        deny=[r'/(en|es)/', r'(calMonth=|calYear=)']
      ),
    callback='process_item',
    follow=True
  ),
)
              </code></pre>
          </section>
        </section>

        <section>
          <h2>Ítems a extraer (items.py)</h2>

          <p>Definimos clase <code>PaxinasItem</code> cos campos que imos a recuperar.</p>

          <img src="assets/custom/images/map.png" data-unbordered="true" alt="" />
        </section>

        <section>
          <h2>Selectores</h2>

          <table>
            <thead>
            <tr>
              <th>CSS (response.css)</th>
              <th>XPath (response.xpath)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
              <td>title::text</td>
              <td>//title/text()</td>
            </tr>
            <tr>
              <td>base::attr(href)</td>
              <td>//base/@href</td>
            </tr>
            <tr>
              <td>a[href*=image]::attr(href)</td>
              <td>//a[contains(@href, "image")]/@href</td>
            </tr>
            </tbody>
          </table>

          <p>Tamén con expresións regulares <code>response.re()</code>.</p>
        </section>

        <section data-background-video="./assets/custom/scrapy-shell.webm" data-background-color="#FFFFFF">
          <h2 class="heading-inverse">scrapy shell http://...</h2>
        </section>

        <section>
          <h2>Implementamos parse_item</h2>
          <p>Empregamos <code>scrapy shell</code> para facer probas dos selectores</p>

          <pre><code class="hljs python" data-trim contenteditable>
class PaxinasSpider(CrawlSpider):
  # ....
  def parse_item(self, response):
    # Comprobacións: 'text/html' not in response.headers['Content-Type']

    item = PaxinasItem()
    item['url'] = response.url
    item['title'] = response.xpath('//title/text()').extract_first()
    item['body'] = response.body
    item['keywords'] = response.xpath('//meta[@name="keywords"]/@content')
            .extract_first()
    # Búsqueda de imaxes e ficheiros
    # ....
    item['image_urls'] = images
    item['files_urls'] = files
    return item
          </code></pre>
        </section>

        <section>
          <h2>Configurar pipelines ficheiros e imaxes</h2>

          <p>Precisamos configurar <code>ITEM_PIPELINES</code> para dispoñer do comportamento automático dos campos <code>image_urls</code> e <code>file_urls</code>.</p>

          <pre><code class="hljs python" data-trim contenteditable>
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 1,
    'scrapy.pipelines.files.FilesPipeline': 1,
}

dir = os.path.dirname(os.path.abspath(__file__))
IMAGES_STORE = dir + '/../assets/images'
FILES_STORE  = dir + '/../assets/files'
          </code></pre>
        </section>

        <section>
          <h2>Executamos crawler</h2>
          <p>Lanzamos para que garde en formato json (-o xerencia.json), con parámetros adicionais (-a), incluímos a opción de parar e reanudar o crawler (-s JOBDIR) e gardamos un log.</p>
          <pre><code class="hljs python" data-trim contenteditable>
scrapy crawl paxinas -o xerencia.json \
    -a start_url='http://www.usc.es/gl/goberno/xerencia/' \
    -a base_path='/goberno/xerencia/' \
    -s JOBDIR=crawls/paxinas-1 \
    --logfile=xerencia.log
          </code></pre>
        </section>

        <section>
          <h2>Mapeo de campos</h2>
          <p>Temos que mapear os campos do documento JSON aos campos do tipo de contido.</p>
          <img src="assets/custom/images/map.png" data-unbordered="true" alt="" />
        </section>

        <section>
          <h2>Estrutura JSON</h2>
          <p>(versión reducida)</p>
          <pre><code class="hljs json">[{
  "title": "Servizo de Xestión Académica - USC",
  "url": "http://www.usc.es/gl/servizos/sxa",
  "keywords": "usc, universidade, santiago , compostela, ...",
  "contidos": "<p>...</p>",
  "image_urls": ["http://www.usc.es/gl/servizos/sxa/imaxes/coie.jpg"],
  "file_urls": ["http://www.usc.es/gl/servizos/sxa/4_Vacantes_Master.pdf"],
  "images": [{
      "url": "http://www.usc.es/gl/servizos/sxa/imaxes/coie.jpg",
      "path": "full/7eb49e6658ab0f2e7ed894027529dfaeebd9346a.jpg"}],
  "files": [{
      "url": "http://www.usc.es/gl/servizos/sxa//4_Vacantes_Master.pdf",
      "path": "full/67063aa5e3f072742ce4d2c066b973d1ceca64c8.pdf"}]
}, {...}]</code></pre>
        </section>

        <section>
          <h2>Integrar en Drupal I</h2>
          <p>Creamos un módulo cun ficheiro <code>module_name.drush.inc</code></p>
          <pre><code class="hljs php" data-trim contenteditable>
/**
 * Implements hook_drush_command().
 */
function usc_front_drush_command() {
  $items['fe-import-pages'] = [
    'callback' => 'usc_front_import_pages',
    'description' => 'Importar páxinas do sitio USC.es (formato JSON).',
    'aliases' => array('feip'),
    'arguments' => [
      'input' => 'Ficheiro JSON co contido das páxinas a cargar.',],
    'options' => [
      'assets' => 'Ruta ao directorio de assets (files e images)',
      'menu_name' => 'Nome do menú a xerar',
    ],
  ];
}</code></pre>
          <aside class="notes">
            - Executar Drush co alias do sitio (@local). Necesario para construír ben as ligazóns.
          </aside>
        </section>

        <section>
          <h2>Integrar en Drupal II</h2>
          <p>Recorrer os ítems do array json e ir inserindo o contido</p>
          <pre><code class="hljs php" data-trim contenteditable>
$data = json_decode(file_get_contents($input), TRUE);
/** @var \Drupal\Core\Path\AliasStorage $aliasStorage */
$aliasStorage = \Drupal::service('path.alias_storage');

foreach($data as $item) {
  if (!$aliasStorage->aliasExists($link, $langcode)) {
    $values = [
      'type' => 'usc_front_page_general',
      'title' => $item['titulo'],
      // ...
      'body' => ['value' => $item['contidos'],],
      'field_fe_description' => $item['description'],
    ];
  }
  // ... tratar imágenes y ficheros
}</code></pre>
        </section>

        <section>
          <h2>Ficheiros xestionados</h2>
          <p>Recorrer arrays de <code>images</code> e <code>files</code>, e damos de alta en Drupal (táboa: file_managed)</p>
          <pre><code class="hljs php" data-trim contenteditable>
foreach($item['images'] as $img) {
  $path = 'public://' . $img['url'];
  $imagename = realpath($assets . '/images/' . $img['path']);
  // validacións omitidas, file_exists, ...
  $image_stream = file_get_contents($imagename);
  if(!file_prepare_directory(dirname($path))) {
    drupal_mkdir($dir, NULL, True);
  }
  $file = file_save_data($image_stream, $path, FILE_EXISTS_REPLACE);
  // $alternativeText = $alt !== null? $alt: t('Imaxe');
  $image_id[] = [
    'target_id' => $file->id(),
    'alt' => $alternativeText,
  ];
  // Remprazar os href no contido ($values['body'])
}
$values['field_fe_image'] = $image_id; // array cos IDs das imáxes
          </code></pre>
        </section>

        <section>
          <h2>Outros campos (taxonomías)</h2>

          <p>Crear taxonomías para o campo <code>keywords</code></p>

          <pre><code class="hljs php" data-trim contenteditable>
$terms = explode(',', $item['keywords']);
foreach($terms as $term) {
  $termId = $storage->getQuery()
    ->condition('vid', $keywords_vid)
    ->condition('name', $term)->execute();

  if (empty($termId)) {
    $termEntity = \Drupal\taxonomy\Entity\Term::create([
      'vid' => $keywords_vid,
      'name' => $term,
    ]);
    $termEntity->save();
    $termId = [$termEntity->id() => $termEntity->id()];
  }
  $tids[] = ['target_id' => current($termId),];
}
$values['field_fe_keywords'] = $tids;</code></pre>
        </section>

        <section>
          <h2>Integrar con menús I</h2>

          <p>Comprobar que existe un menú, senón crealo</p>

          <pre><code class="hljs php" data-trim contenteditable>
// $menu_name → nome do menú
$nids = \Drupal::entityQuery('menu')
  ->condition('id', $menu_name)->execute();

if (empty($nids)) { // Crear menú si no existe
  $menu_desc = t('Menú') . ' ' . ucwords(str_replace('menu-', '', $menu_name));
  /** @var \Drupal\system\Entity\Menu $menu */
  $menu = entity_create('menu', array(
    'id'          => $menu_name,
    'label'       => $menu_desc,
    'description' => $menu_desc,
    'langcode'    => 'gl',
  ));
  $menu->save();
}
</code></pre>
        </section>

        <section>
          <h2>Integrar con menús II</h2>

          <p>Crear ítems do menú</p>

          <pre><code class="hljs php" data-trim contenteditable>
// Comprobar se xa existe
$nids = \Drupal::entityQuery('menu_link_content')
  ->condition('default_langcode', 1)->condition('title', $item['titulo'])
  ->condition('link.uri', 'internal:' . $link)
  ->condition('menu_name', $menu_name)->execute();

if (count($nids) === 0) {
  $menuData = [
    'title' => $item['titulo'],
    'menu_name' => $menu_name,
    // ...
    'link' => ['uri' => 'internal:' . $link], ];

  $menu_link_content = MenuLinkContent::create($menuData);
  $menu_link_content->save();
}
</code></pre>
        </section>

        <section>
          <h2>Outras alternativas</h2>
          <p>
            <ul>
              <li class="fragment fade-in">Empregar o módulo migrate para cargar os contidos.</li>
              <li class="fragment fade-in">Insertar dende código Python os contidos directamente na base de datos.</li>
              <li class="fragment fade-in">Publicar os servizos REST de Drupal e dende código Python insertar vía REST.</li>
            </ul>
          </p>
        </section>

        <section>
          <h2>BONUS: Implementar pipeline propio</h2>
          <pre><code class="hljs python" data-trim contenteditable>
import pymongo
# import ...
class PaxinasPipeline(object):
  def __init__(self):
    connection = pymongo.MongoClient(
      settings['MONGODB_SERVER'],
      settings['MONGODB_PORT']
    )
    db = connection[settings['MONGODB_DB']]
    self.collection = db[settings['MONGODB_COLLECTION']]

  def process_item(self, item, spider):
    for data in item:
      if not data:
        raise DropItem("Missing data!")
    self.collection.update({'url': item['url']}, dict(item), upsert=True)
    return item
</code></pre>
          <p></p>
        </section>

        <section>
          <h2>BONUS: Configuramos pipeline MongoDB</h2>

          <pre><code class="hljs python" data-trim contenteditable>
ITEM_PIPELINES = {
  'scrapy.pipelines.images.ImagesPipeline': 1,
  'scrapy.pipelines.files.FilesPipeline': 1,
  'paxinas.pipelines.PaxinasPipeline': 1,
}

dir = os.path.dirname(os.path.abspath(__file__))
IMAGES_STORE = dir + '/../assets/images'
FILES_STORE  = dir + '/../assets/files'

MONGODB_SERVER = "localhost"
MONGODB_PORT = 27017
MONGODB_DB = "usc"
MONGODB_COLLECTION = "paxinas"
</code></pre>
        </section>

        <section>
          <h2>Portia</h2>
          <p>Portia, proxecto que permite extraer de forma visual.</p>
        </section>

        <section>
          <h2>Faino ti mesmo</h2>

          <p>
            <img src="assets/custom/images/Octocat.jpg" data-unbordered="true" alt="" width="100" />
          </p>

          <p class="fragment fade-in">
            Esta presentación está dispoñible en: <a href="https://vifito.github.io/scrapy-for-drupal/">vifito.github.io/scrapy-for-drupal/</a>
          </p>

          <p class="fragment fade-in">
            E no repositorio: <a href="https://github.com/vifito/scrapy-for-drupal">github.com/vifito/scrapy-for-drupal</a>
          </p>

          <pre class="fragment fade-in"><code data-trim contenteditable>
github.com/vifito/scrapy-for-drupal
└── demo
    ├── module
    │   └── scrapy
    └── scrapy
        └── scrapy

            </code></pre>
        </section>

        <section>
          <h2>Faino ti mesmo</h2>

          <p>
            A demo inclúe <strong>unha araña con Scrapy</strong> que recupera os artigos de Drupal 8 de Sitepoint (https://www.sitepoint.com/tag/drupal-8/). E un <strong>módulo de Drupal</strong> para insertar os artigos.
          </p>

          <p>No directorio <code>scrapy</code> executar <code>launch.sh</code> ou:</p>
          <pre><code class="hljs bash" data-trim contenteditable>
scrapy crawl sitepoint -o drupal.json --logfile=drupal.log
            </code></pre>

          <p>Copiar o ficheiro <code>drupal.json</code> e o directorio <code>assets</code> á raíz dunha instalación drupal e executar::</p>
          <pre><code class="hljs bash" data-trim contenteditable>
drush gdgourense-import drupal.json --assets=./assets
            </code></pre>
        </section>


        <section style="text-align: left;">
          <h1>FIN</h1>
          <p>
            Visítanos en breve
            <a href="http://www.usc.es">http://www.usc.gal</a>
          </p>
          <p class="text-center">
            <img src="assets/custom/images/usc-love-drupal.png" align="center" data-unbordered="true" alt="" />
          </p>
        </section>

      </div>

    </div>

    <script src="assets/reveal.js/lib/js/head.min.js"></script>
    <script src="assets/reveal.js/js/reveal.js"></script>

    <script>

      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: 'c/t',

        transition: 'concave', // none/fade/slide/convex/concave/zoom

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'assets/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'assets/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'assets/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'assets/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'assets/reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'assets/reveal.js/plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
